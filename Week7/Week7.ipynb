{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 1: Clustering based on multinomial mixture model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as sps\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import Binarizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. Initialize k jk random.\n",
    " 2. For t = 12\n",
    " (a) For = 1\n",
    " (b) For k = 1\n",
    " (c) For k = 1\n",
    " N compute k = argmaxkp(x k)\n",
    " K compute Nk. Set k := NkN\n",
    " K j=1 dcomputemjk. Set jk =mjk\n",
    "\"\"\"\n",
    "\n",
    "class MixtureModel:\n",
    "    \"\"\"\n",
    "    Class for making a Multinomial Mixture model for clustering data.\n",
    "    \"\"\"\n",
    "    def __init__(self, clusters:int, features:int, data):\n",
    "        self.clusters = clusters\n",
    "        self.features = features \n",
    "        self.data = data\n",
    "\n",
    "        self.pi = np.random.rand(clusters)\n",
    "        self.pi /= self.pi.sum()\n",
    "        self.pi = np.reshape(self.pi, (10,1))\n",
    "\n",
    "        self.mu_jk = np.random.rand(clusters, features)\n",
    "\n",
    "    def logprob_data_given_clusters(self):\n",
    "        \"\"\"\n",
    "        Calculates the log probability of each data point given each cluster\n",
    "        \"\"\"\n",
    "        epsilon = 1e-10\n",
    "        self.mu_jk = np.clip(self.mu_jk, epsilon, 1 - epsilon)\n",
    "        pxsk = self.data@np.log(self.mu_jk).T + (1-self.data)@np.log(1-self.mu_jk).T\n",
    "        return pxsk\n",
    "    \n",
    "        \n",
    "    def k_mu(self):\n",
    "        \"\"\"\n",
    "        Computes the log probability of each data point given each cluster, then takes the arg-max over the clusters.\n",
    "        \"\"\"\n",
    "        pxk = np.log(self.pi).T + self.logprob_data_given_clusters()\n",
    "        return np.argmax(pxk, axis=1, keepdims=True)\n",
    "    \n",
    "    def delta_func(self, k, kmu):\n",
    "        \"\"\"\n",
    "        Returns a vector corresponding to the dirac delta function for the assignation of data points to clusters (kmu), and the clusters.\n",
    "        \"\"\"\n",
    "        ks = np.full_like(kmu, k)\n",
    "        return np.where(ks == kmu, 1, 0)\n",
    "    \n",
    "    def N_k(self, kmu):\n",
    "        \"\"\"\n",
    "        Computes the number of data points assigned to each cluster k\n",
    "        \"\"\"\n",
    "        N_k = np.ones((self.clusters, 1))\n",
    "        for k in range(self.clusters):\n",
    "            N_k[k] = np.sum(self.delta_func(k, kmu))\n",
    "        return N_k\n",
    "    \n",
    "    def set_pi_k(self, N_k):\n",
    "        \"\"\"\n",
    "        Updates the parameter for pi_k\n",
    "        \"\"\"\n",
    "        self.pi  = (N_k+1)/(self.data.shape[0] +1)\n",
    "    \n",
    "    def m_jk(self, N_k, kmu):\n",
    "        \"\"\"\n",
    "        Computes updated parameters mu_jk\n",
    "        \"\"\"\n",
    "        m = np.zeros_like(self.mu_jk)\n",
    "        for j in range(self.features):\n",
    "            for k in range(self.clusters):\n",
    "                m[k, j] = np.sum(self.data[:,j]*self.delta_func(k, kmu).T)\n",
    "\n",
    "        m = m/N_k\n",
    "        m = np.nan_to_num(m)\n",
    "        m = np.where(m==0, np.random.rand(self.clusters, self.features), m)\n",
    "\n",
    "        return m\n",
    "    \n",
    "    def plot(self):\n",
    "        plt.figure(figsize=(10, 10))\n",
    "    \n",
    "        for i in range(self.clusters):\n",
    "            image = self.mu_jk[i].reshape((28,28))\n",
    "            # denormalization\n",
    "            image = image*255\n",
    "\n",
    "            plt.subplot(1, self.clusters, i + 1)\n",
    "            plt.imshow(image, cmap='gray')\n",
    "            plt.title(f'Cluster {i+1}')\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    def run_model(self, max_iter = 1000):\n",
    "        \"\"\"\n",
    "        Runs the Multinomial mixture algorithm on the data\n",
    "        \"\"\"\n",
    "        for t in range(max_iter):\n",
    "            k_mu = self.k_mu()\n",
    "            # Diagnostic for argmax function:\n",
    "            unique, counts = np.unique(k_mu, return_counts=True)\n",
    "            # print(dict(zip(unique, counts)))\n",
    "\n",
    "            N_k = self.N_k(k_mu)\n",
    "            m_jk = self.m_jk(N_k, k_mu)\n",
    "            self.set_pi_k(N_k)\n",
    "            self.mu_jk = m_jk\n",
    "        self.plot()\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Femke\\anaconda3\\Lib\\site-packages\\sklearn\\datasets\\_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X = mnist.data.astype(np.int32).values\n",
    "y = mnist.target.astype(np.int32)\n",
    "\n",
    "binarizer = Binarizer(threshold=127)\n",
    "X_bin = binarizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MM = MixtureModel(10, X.shape[1], X)\n",
    "MM.run_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 3: EM algorithm on Old Faithful Geyser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gaussian(mu, cov, ax, color):\n",
    "    x = np.linspace(-3, 3, 100)\n",
    "    y = np.linspace(-3, 3, 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    pos = np.empty(X.shape + (2,))\n",
    "    pos[:, :, 0] = X\n",
    "    pos[:, :, 1] = Y\n",
    "\n",
    "    # Define the multivariate normal distributions\n",
    "    rv1 = multivariate_normal(mu, cov)\n",
    "\n",
    "    # Calculate the PDF (probability density function) over the grid\n",
    "    Z1 = rv1.pdf(pos)\n",
    "\n",
    "    ax.contour(X, Y, Z1, levels=[0.1], colors=color)\n",
    "\n",
    "\n",
    "def plot_data(data, mu, cov):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(data[:, 0], data[:, 1], s=10, c='green', label='Samples')\n",
    "\n",
    "    plot_gaussian(mu[0], cov[0], ax, color='blue')\n",
    "    plot_gaussian(mu[1], cov[1], ax, color='red')\n",
    "\n",
    "    # Set the plot limits and labels\n",
    "    ax.set_xlim(-2.5, 2.5)\n",
    "    ax.set_ylim(-2.5, 2.5)\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_classified_data(data, mu, cov, r_k):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Assign each point to the cluster with the highest responsibility\n",
    "    cluster_assignments = np.argmax(r_k, axis=0)\n",
    "    cluster_0 = data[cluster_assignments == 0]\n",
    "    cluster_1 = data[cluster_assignments == 1]\n",
    "\n",
    "    # Plot colored sata points\n",
    "    ax.scatter(cluster_0[:, 0], cluster_0[:, 1], s=20, c='blue', label='Cluster 0')\n",
    "    ax.scatter(cluster_1[:, 0], cluster_1[:, 1], s=20, c='red', label='Cluster 1')\n",
    "\n",
    "    # Plot the Gaussian distributions as contours\n",
    "    plot_gaussian(mu[0], cov[0], ax, color='blue')\n",
    "    plot_gaussian(mu[1], cov[1], ax, color='red')\n",
    "\n",
    "    ax.set_xlim(-2.5, 2.5)\n",
    "    ax.set_ylim(-2.5, 2.5)\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def prep_data():\n",
    "    data = pd.read_csv('old_faithful_geyser.txt', delim_whitespace=True, header=None, index_col=0).to_numpy()\n",
    "\n",
    "    mean = np.mean(data, axis=0)\n",
    "    std_dev = np.std(data, axis=0)\n",
    "\n",
    "    # Perform the standardization\n",
    "    standardized_data = (data - mean) / std_dev\n",
    "    return standardized_data\n",
    "\n",
    "\n",
    "def gaussian_with_pi_k(pi_probability, big_sigma, a, X):\n",
    "    \"\"\"\n",
    "    Calculates the joint probability of the prior for the classes and the multivariate Gaussian for the data for that class\n",
    "\n",
    "    :param pi_probability: prior for the classes\n",
    "    :param big_sigma: covariance matrix for the Gaussian\n",
    "    :param a: mean vector for the Gaussian\n",
    "    :param X: data points\n",
    "    \"\"\"\n",
    "    return pi_probability * multivariate_normal.pdf(X, mean=a, cov=big_sigma)\n",
    "\n",
    "def multi_cluster_gaussian_with_pi_k(pi_k, big_sigma_k, a_k, X, k):\n",
    "    gaussians_with_pi = []\n",
    "    # calculating enumerators of both clusters of responsibilities\n",
    "    for i in range(k):\n",
    "        gaussians_with_pi.append(gaussian_with_pi_k(pi_k[i], big_sigma_k[i], a_k[i], X))\n",
    "    return gaussians_with_pi\n",
    "\n",
    "\n",
    "def E_step(pi_k, big_sigma_k, X, a_k, k):\n",
    "    gaussians_with_pi = multi_cluster_gaussian_with_pi_k(pi_k, big_sigma_k, a_k, X, k)\n",
    "\n",
    "    # calculating responsibilities for both clusters\n",
    "    r_k = []\n",
    "    for gaussian_pi in gaussians_with_pi:\n",
    "        r_k.append(gaussian_pi / sum(gaussians_with_pi))\n",
    "\n",
    "    return r_k\n",
    "\n",
    "\n",
    "def M_step(r_k, X):\n",
    "    # I always reference with k if that contains information about both clusters,\n",
    "    # otherwise I use letter without k\n",
    "    n = len(X)\n",
    "    pi_k = []\n",
    "    big_sigma_k = []\n",
    "    a_k = []\n",
    "    # iterate over clusters\n",
    "    for i, r in enumerate(r_k):\n",
    "        sum_of_r = sum(r)\n",
    "\n",
    "        # updating pi\n",
    "        new_pi = sum_of_r / n\n",
    "        pi_k.append(new_pi)\n",
    "\n",
    "        # updating mean\n",
    "        new_a = (1 / sum_of_r) * (r @ X)\n",
    "        a_k.append(new_a)\n",
    "\n",
    "        # updating variance\n",
    "        diff = X - new_a\n",
    "        weighted_diff = diff.T * r\n",
    "        new_cov = np.dot(weighted_diff, diff) / sum_of_r\n",
    "        big_sigma_k.append(new_cov)\n",
    "\n",
    "    return pi_k, big_sigma_k, a_k\n",
    "\n",
    "def calc_likelihood(pi_k, big_sigma_k, a_k, X, k):\n",
    "    gaussians_with_pi = multi_cluster_gaussian_with_pi_k(pi_k, big_sigma_k, a_k, X, k)\n",
    "    log_likelihood = sum(np.log(sum(gaussians_with_pi)))\n",
    "    return log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting main code:\n",
    "data = prep_data()\n",
    "\n",
    "# initialization of values\n",
    "k = 2\n",
    "pi_k = np.array([1 / k, 1 / k])\n",
    "\n",
    "# starting from the values that were given in the book so that we have a matching graph\n",
    "# mu = np.array([[-1, 1], [1, -1]])\n",
    "mu = np.array([[-1.5, 1.5], [1.5, -1.5]])\n",
    "cov = np.array([np.eye(2), np.eye(2)])\n",
    "\n",
    "# visualizing initial data\n",
    "plot_data(data, mu, cov)\n",
    "\n",
    "epsilon = 0.00001\n",
    "previous_likelihood = 0\n",
    "# EM iteration\n",
    "for i in range(60):\n",
    "    responsibilities = E_step(pi_k, cov, data, mu, k)\n",
    "    pi_k, cov, mu = M_step(responsibilities, data)\n",
    "    likelihood = calc_likelihood(pi_k, cov, mu, data, k)\n",
    "    delta = abs(likelihood - previous_likelihood)\n",
    "    previous_likelihood = likelihood\n",
    "    print(delta)\n",
    "    if epsilon >= delta:\n",
    "        break\n",
    "\n",
    "print(f'stopped after iteration {i+1}')\n",
    "#\n",
    "plot_classified_data(data, mu, cov, responsibilities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
